# Scheduling

Scheduling 是指如何安排 Pod 到哪個 Node 上。

在 pod 的 yaml 中，spec.nodeName 可以指定 pod 要跑在哪個 node 上。預設沒有這個 key，所以剛創建的 pod 會是 pending 的狀態。

Kubernetes 發現沒有這個 key 的 pod 後，會按照 scheduling algorithm 來安排 pod 放到哪個 node 上，並加入 nodeName。

## Manual Scheduling

如果不想要用 Kubernetes 自動安排 pod ，則根據以下兩種情況，可以手動指定 pod 要跑在哪個 node 上：

- 創建 pod
    - 在 yaml 中 spec.nodeName 指定 nodeName
- 已運行的 pod
    - 用 Pod 的 Binding API 提交 nodeName 變更

Binding API 的範例：

```yaml
apiVersion: v1
kind: Binding
metadata:
  name: mypod
target:
  apiVersion: v1
  kind: Pod
  name: mypod
  namespace: default
```

```
curl -k -X POST http://$server:$port/api/v1/namespaces/default/pods/mypod/binding -H "Content-Type: application/json" -d '{"apiVersion":"v1","kind":"Binding","metadata":{"name":"mypod"},"target":{"apiVersion":"v1","kind":"Pod","name":"mypod","namespace":"default"}}'
```

如果 describe pod 發現 pod 的 nodeName 是空的，則可能還沒有指派 pod 到 node 上。有可能是資源或權限的關係。


## Taints and Tolerations

Taint 是設定在 node 上，讓一般的 pod 不會被安排到這個 node 上。

Toleration 是設定在 pod 上，讓 pod 可以被安排到有 taint 的 node 上。

這個功能可以讓某些 node 專門跑某些特定的 pod，例如：

- 專門跑 GPU 的 node
- 專門跑高記憶體的 node
- 專門跑高 CPU 的 node

或是當 node 有問題時，把問題的 node 標記起來，讓 pod 不會被安排到這個 node 上。

Taint 有三種 effect: 

1. NoSchedule: 不會把 pod 安排到這個有 taint 的 node 上。除非 pod 有 Toleration。
2. PreferNoSchedule: 盡量不會把 pod 安排到這個有 taint 的 node 上。
3. NoExecute: 不會把 pod 安排到有 taint 的 node 上，並且會把已經在 node 上沒有 Toleration 的 pod 移除。

要注意，Toleration 不是說這個 Pod 一定可以被安排在對應 taint 的 node，而是說可以被安排在對應 taint 的 node 上。

## Node Selector

Node Selector 讓工程師可以將特定的 pod 跑在特定的 node 上，讓工程師可以分配及隔離資源。

透過 pod 的 nodeSelector，Kubernetes 尋找有符合匹配 tag 的 node 部屬 pod。

用法

1. 再 Node 加上標籤

```
kubectl label nodes <node-name> <key>=<value>
kubectl label nodes node-1 disktype=ssd
```

2. Pod Spec 定義 nodeSelector

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: nginx
  nodeSelector:
    disktype: ssd
```

## Node Affinity

Node Affinity 可以更進階地告訴 Kubernetes 怎麼將 pod 分配在節點上。

透過設定強制或偏好條件，並利用 [operators](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators) 建立分配時的規則規則。

用法

```
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - us-west
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
 ```

| Scheduling\Execution | required | ignore                                                                                  |
| -------------------- | -------- | --------------------------------------------------------------------------------------- |
| required             | 還在規劃 | requiredDuringSchedulingIgnoredDuringExecution<br>部屬 pod 時，Node 一定要符合條件      |
| preffered            |          | preferredDuringSchedulingIgnoredDuringExecution<br>部屬 pod 時，Node **儘量**要符合條件 |

## Resource Requirements and limits

藉由 request 和 limits 的設定，分配合適的資源給 pod 使用。最好的情況是 pod 不會因為競爭資源導致有些 pod 沒資源，或是機器有大量資源閒置資源沒有使用。

在 pod 的 yaml 

```yaml
resources:
  requests:
    memory: "128Mi"
    cpu: "500m" # 1 vCPU = 1000m, so 500m = 0.5 vCPU
  limits:
    memory: "256Mi"
    cpu: "1"
```

|            | No Limit                                                                                                                                                                                  | Limit                                                                                  |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| No Requets | 某一個 pod 把所有資源吃掉，讓其他 pod 不能部屬。但其他 pod 因為沒有註明要多少資源而發生無法部屬情況                                                                                       | Kubernetes 會自動將 request 設定成跟 limit 一樣                                        |
| Requests   | (最理想)<br>"每個 Pod 可以消耗可用的資源超過 request，但當其他 Pod 需要資源時，系統不會基於 request 強制限制已經使用過多資源的 Pod，這可能會導致競爭資源的情況，最終由 Kubelet 調整分配。" by ChatGPT | 如果某個 pod 要更多資源，而其中一個 pod 不需要這麼多，就會有閒置資源情況。 |

Kubernetes 不會預設 request 和 limit，但我們可以裡用 LimitRange 設定預設值讓每個 pod 都有預設值。

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: default
spec:
  limits:
    - type: Container
           
      default:                # limit
        cpu: "500m"             # 預設 500m CPU
        memory: "256Mi"         # 預設 256Mi 記憶體
      defaultRequest:         # request
        cpu: "200m"             # 預設 request 200m CPU
        memory: "256Mi"         # 預設 request 256Mi 記憶體
      max:                    # limit
        cpu: "1"                # 最大 1 CPU
        memory: "512Mi"         # 最大 512Mi 記憶體
      min:                    # request
        cpu: "100m"             # 最小 100m CPU
        memory: "128Mi"         # 最小 128Mi 記憶體
      maxLimitRequestRatio:
        cpu: "2"                # 限制 limit/request 比例為 2
```

- max：設定容器的資源上限。此範例中，容器最多可以使用 1 個 CPU 和 512Mi 記憶體。
- min：設定容器的資源下限。此範例中，容器至少要請求 100m CPU 和 128Mi 記憶體
- maxLimitRequestRatio：設置 limit 和 request 的最大比例，避免 Pod 設置過高的 limits 卻僅使用少量資源。

|                             | Pod specify Request               | Pod does not specify Requests                   |
| --------------------------- | --------------------------------- | ----------------------------------------------- |
| Pod specify Limits          | 按照 pod spec                     | （注意！）Pod 的 Request 被設定成跟 Limits 一樣 |
| Pod does not specify Limits | Pod 的 Limits 採用 default Limits | Pod apply both default Requests and Limits      |

補充

1. 檢查階段：用 requests 和 limits 來檢查 namespace 的配額，這部分是正確的。你可以通過這些參數來評估目前資源的使用情況。

2. 調度階段：用 requests 分配資源，這也是正確的。當 pod 被調度時，Kubernetes 會根據 requests 來確保節點有足夠的資源來滿足 pod 的需求。

3. 運行階段：用 limits 設定最大用量，這是對的。當一個 pod 超過設定的 limits 時，Kubernetes 會觸發 OOMKill（Out Of Memory Kill），終止該 pod 的運行。


## DaemonSets

當我們想要在每一個節點都部屬某些特定 pod 時（如 搜集 log 的 pod），就可以使用 DaemonSets。

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-collector
spec:
  selector:
    matchLabels:
      app: log-collector
  template:
    metadata:
      labels:
        app: log-collector
    spec:
      containers:
      - name: log-collector
        image: log-collector-image:latest
```

## Static Pods


當算沒有 master node ，每個 worker node 也可以設定及管理 pod(基本上每個 kubelet 也只能管理到 pod level)。通常重要組件的 pod 會是用這種方式管理，這些 pod 也稱為 static pod。Master Node 裡面的組件像是 api server, etcd 也是這樣管理。

Kubelet 會定期掃描特定資料夾，通常位置是 `/etc/kubernetes/manifests` （要注意可能因為設定不同，manifests 放在其他地方），根據這個資料夾裡的 yaml 進行創建或更新 Pod。

Kubelet 創建 static pod 後，如果該 pod 屬於集群，Kubelet 會在 API server 中顯示一個對應的鏡像 Pod，但無法像普通 Pod 一樣透過 API 進行編輯或刪除。要修改還是要回資料夾下的 yaml 更新。

Static Pod 和 DaemonSets 創建的 pod 都會被 kube-schedule 忽略。

## Multiple Schedulers

Scheduler 也可以客製化，根據不同的需求，建立不同規則的 Scheduler 。

部屬其他 Scheduler 流程跟部屬 pod 一樣。而有需要被特定 scheduler 管理的 pod 則是要在 yaml 中指定 scheduler 的名稱。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: custom-scheduler-pod
spec:
  schedulerName: custom-scheduler # 指定 schedulerName
  containers:
  - name: nginx
    image: nginx
```

## Configuring Scheduler Profiles

Sheduler 安排 pod 時，主要有四個步驟：

1. Scheduling queue
    - 當 pod 被創建時，會先被放入這個 queue 中。
2. Filtering
    - Scheduler 根據一些預先定義的條件（例如資源需求、親和性/反親和性、Node Taints 和 Tolerations）篩選出符合要求的節點。
3. Scoring
    - 對篩選後的節點進行評分，以確定最合適的節點來運行 Pod。
4. Binding
    - 一旦選定了最佳節點，調度器會將 Pod 綁定到該節點上。

每個步驟都有各種 extension points 去計算與規劃 pod ，使用者也可以去客製化 extension points 以客製化安排 Pod 的規則。

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - name: high-priority
    schedulerName: high-priority-scheduler
    plugins:
      score:
        enabled:
          - name: NodePreferAvoidPods
          - name: NodeAffinity
      filter:
        enabled:
          - name: NodeSelector
          - name: NodeUnschedulable
    plugins:
      bind:
        enabled:
          - name: DefaultBinder

  - name: low-priority
    schedulerName: low-priority-scheduler
    plugins:
      score:
        enabled:
          - name: NodeResourcesBalancedAllocation
          - name: TaintToleration
      filter:
        enabled:
          - name: NodePorts
          - name: PodToleratesNodeTaints
    plugins:
      bind:
        enabled:
          - name: DefaultBinder

  - name: custom-scheduler
    schedulerName: custom-scheduler
    plugins:
      score:
        enabled:
          - name: CustomScorer
      filter:
        enabled:
          - name: CustomFilter
    plugins:
      bind:
        enabled:
          - name: DefaultBinder
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority-class
value: 1000000
globalDefault: false
description: "This priority class is for high priority workloads."

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority-class
value: 500000
globalDefault: false
description: "This priority class is for low priority workloads."
```

- profiles:
	-	high-priority：這個 Profile 專為高優先級工作負載設計，使用 NodePreferAvoidPods 和 NodeAffinity 進行評分，並在過濾階段使用 NodeSelector 和 NodeUnschedulable。
	-	low-priority：此 Profile 專為低優先級工作負載設計，使用 NodeResourcesBalancedAllocation 和 TaintToleration 進行評分，並使用 NodePorts 和 PodToleratesNodeTaints 進行過濾。
	-	custom-scheduler：這是用於自定義調度的 Profile，可以使用自定義的過濾和評分插件。
- PriorityClass：這些資源定義了不同優先級的工作負載，幫助確保高優先級的 Pods 在資源緊張的情況下可以被調度。