# Scheduling

Scheduling 是指如何安排 Pod 到哪個 Node 上。

在 pod 的 yaml 中，spec.nodeName 可以指定 pod 要跑在哪個 node 上。預設沒有這個 key，所以剛創建的 pod 會是 pending 的狀態。

Kubernetes 發現沒有這個 key 的 pod 後，會按照 scheduling algorithm 來安排 pod 放到哪個 node 上，並加入 nodeName。

## Manual Scheduling

如果不想要用 Kubernetes 自動安排 pod ，則根據以下兩種情況，可以手動指定 pod 要跑在哪個 node 上：

- 創建 pod
    - 在 yaml 中 spec.nodeName 指定 nodeName
- 已運行的 pod
    - 用 Pod 的 Binding API 提交 nodeName 變更

Binding API 的範例：

```yaml
apiVersion: v1
kind: Binding
metadata:
  name: mypod
target:
  apiVersion: v1
  kind: Pod
  name: mypod
  namespace: default
```

```
curl -k -X POST http://$server:$port/api/v1/namespaces/default/pods/mypod/binding -H "Content-Type: application/json" -d '{"apiVersion":"v1","kind":"Binding","metadata":{"name":"mypod"},"target":{"apiVersion":"v1","kind":"Pod","name":"mypod","namespace":"default"}}'
```

如果 describe pod 發現 pod 的 nodeName 是空的，則可能還沒有指派 pod 到 node 上。有可能是資源或權限的關係。


## Taints and Tolerations

Taint 是設定在 node 上，讓一般的 pod 不會被安排到這個 node 上。

Toleration 是設定在 pod 上，讓 pod 可以被安排到有 taint 的 node 上。

這個功能可以讓某些 node 專門跑某些特定的 pod，例如：

- 專門跑 GPU 的 node
- 專門跑高記憶體的 node
- 專門跑高 CPU 的 node

或是當 node 有問題時，把問題的 node 標記起來，讓 pod 不會被安排到這個 node 上。

Taint 有三種 effect: 

1. NoSchedule: 不會把 pod 安排到這個有 taint 的 node 上。除非 pod 有 Toleration。
2. PreferNoSchedule: 盡量不會把 pod 安排到這個有 taint 的 node 上。
3. NoExecute: 不會把 pod 安排到有 taint 的 node 上，並且會把已經在 node 上沒有 Toleration 的 pod 移除。

要注意，Toleration 不是說這個 Pod 一定可以被安排在對應 taint 的 node，而是說可以被安排在對應 taint 的 node 上。

## Node Selector

Node Selector 讓工程師可以將特定的 pod 跑在特定的 node 上，讓工程師可以分配及隔離資源。

透過 pod 的 nodeSelector，Kubernetes 尋找有符合匹配 tag 的 node 部屬 pod。

用法

1. 再 Node 加上標籤

```
kubectl label nodes <node-name> <key>=<value>
kubectl label nodes node-1 disktype=ssd
```

2. Pod Spec 定義 nodeSelector

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: nginx
  nodeSelector:
    disktype: ssd
```

## Node Affinity

Node Affinity 可以更進階地告訴 Kubernetes 怎麼將 pod 分配在節點上。

透過設定強制或偏好條件，並利用 [operators](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators) 建立分配時的規則規則。

用法

```
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: region
            operator: In
            values:
            - us-west
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
 ```

| Scheduling\Execution | required | ignore                                                                                  |
| -------------------- | -------- | --------------------------------------------------------------------------------------- |
| required             | 還在規劃 | requiredDuringSchedulingIgnoredDuringExecution<br>部屬 pod 時，Node 一定要符合條件      |
| preffered            |          | preferredDuringSchedulingIgnoredDuringExecution<br>部屬 pod 時，Node **儘量**要符合條件 |

## Resource Requirements and limits

藉由 request 和 limits 的設定，分配合適的資源給 pod 使用。最好的情況是 pod 不會因為競爭資源導致有些 pod 沒資源，或是機器有大量資源閒置資源沒有使用。

在 pod 的 yaml 

```yaml
resources:
  requests:
    memory: "128Mi"
    cpu: "500m" # 1 vCPU = 1000m, so 500m = 0.5 vCPU
  limits:
    memory: "256Mi"
    cpu: "1"
```

|            | No Limit                                                                                            | Limit                                                                      |
| ---------- | --------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| No Requets | 某一個 pod 把所有資源吃掉，讓其他 pod 不能部屬。但其他 pod 因為沒有註明要多少資源而發生無法部屬情況 | Kubernetes 會自動將 request 設定成跟 limit 一樣                            |
| Requests   | (最理想)<br>"每個 Pod 可以消耗可用的資源超過 request，但當其他 Pod 需要資源時，系統不會基於 request 強制限制已經使用過多資源的 Pod，這可能會導致競爭資源的情況，最終由 Kubelet 調整分配。| 如果某個 pod 要更多資源，而其中一個 pod 不需要這麼多，就會有閒置資源情況。" by ChatGPT |

Kubernetes 不會預設 request 和 limit，但我們可以裡用 LimitRange 設定預設值讓每個 pod 都有預設值。

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limits
  namespace: default
spec:
  limits:
    - type: Container
           
      default:                # limit
        cpu: "500m"             # 預設 500m CPU
        memory: "256Mi"         # 預設 256Mi 記憶體
      defaultRequest:         # request
        cpu: "200m"             # 預設 request 200m CPU
        memory: "256Mi"         # 預設 request 256Mi 記憶體
      max:                    # limit
        cpu: "1"                # 最大 1 CPU
        memory: "512Mi"         # 最大 512Mi 記憶體
      min:                    # request
        cpu: "100m"             # 最小 100m CPU
        memory: "128Mi"         # 最小 128Mi 記憶體
      maxLimitRequestRatio:
        cpu: "2"                # 限制 limit/request 比例為 2
```

- default：如果容器的規範中沒有設置 limits，則會應用這裡的預設值。
- defaultRequest：如果沒有設置 requests，則會應用這裡的預設值
- max：設定容器的資源上限。此範例中，容器最多可以使用 1 個 CPU 和 512Mi 記憶體。
- min：設定容器的資源下限。此範例中，容器至少要請求 100m CPU 和 128Mi 記憶體
- maxLimitRequestRatio：設置 limit 和 request 的最大比例，避免 Pod 設置過高的 limits 卻僅使用少量資源。


